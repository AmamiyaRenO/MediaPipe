using UnityEngine;
using System;
using System.Collections.Generic;
using System.Linq;

/// <summary>
/// Voice Emotion Analyzer - Analyzes user emotion state based on voice characteristics
/// </summary>
public class VoiceEmotionAnalyzer : MonoBehaviour
{
    [Header("=== Analysis Configuration ===")]
    [Tooltip("Enable voice emotion analysis")]
    public bool enableVoiceAnalysis = true;
    
    [Tooltip("Audio data buffer size")]
    public int bufferSize = 1024;
    
    [Tooltip("Data validity period (seconds)")]
    public float dataValidityPeriod = 5.0f;
    
    [Header("=== Emotion Detection Thresholds ===")]
    [Tooltip("Volume threshold for voice detection (0.01-0.1)")]
    public float volumeThreshold = 0.02f; // é™ä½é˜ˆå€¼æé«˜çµæ•åº¦
    
    [Tooltip("High frequency energy threshold (excitement detection)")]
    public float highFreqThreshold = 0.5f;
    
    [Tooltip("Minimum speech frames for validation")]
    public int minSpeechFrames = 3; // æ–°å¢ï¼šéœ€è¦è¿ç»­å¸§æ‰ç®—æœ‰æ•ˆè¯­éŸ³
    
    [Tooltip("Speech rate detection window size")]
    public int speechRateWindow = 10;
    
    [Header("=== Component Dependencies ===")]
    [Tooltip("Voice processor component")]
    public VoiceProcessor voiceProcessor;
    
    [Tooltip("Vosk speech recognizer")]
    public VoskSpeechToText voskSpeech;
    
    [Header("=== Debug Options ===")]
    [Tooltip("Show analysis logs in console")]
    public bool showAnalysisLogs = false;
    
    [Tooltip("Display voice analysis data on UI")]
    public bool showVoiceUI = true;
    
    public Action<EmotionDetectionSystem.EmotionData> OnVoiceEmotionDetected;
    
    // éŸ³é¢‘æ•°æ®ç›¸å…³
    private float[] audioBuffer;
    private List<float> volumeHistory = new List<float>();
    private List<float> pitchHistory = new List<float>();
    private List<float> speechRateHistory = new List<float>();
    
    // è¯­éŸ³æ´»åŠ¨æ£€æµ‹ç›¸å…³ - æ–°å¢
    private List<bool> voiceActivityHistory = new List<bool>();
    private int consecutiveSilenceFrames = 0;
    private int consecutiveVoiceFrames = 0;
    private float noiseFloor = 0.01f;
    
    // åˆ†æç»“æœ
    private EmotionDetectionSystem.EmotionData currentVoiceEmotion;
    private DateTime lastAnalysisTime;
    private bool hasRecentData = false;
    
    // å†…éƒ¨åˆ†æå‚æ•°
    private float currentVolume = 0f;
    private float currentPitch = 0f;
    private float currentSpeechRate = 0f;
    private float averageVolume = 0f;
    private float volumeVariance = 0f;
    
    // é¢‘è°±åˆ†æç›¸å…³
    private float[] spectrum = new float[256];
    private float lowFreqEnergy = 0f;
    private float midFreqEnergy = 0f;
    private float highFreqEnergy = 0f;
    
    // è¯­éŸ³è¯†åˆ«ç›¸å…³
    private List<DateTime> speechEventTimes = new List<DateTime>();
    private string lastRecognizedText = "";
    private DateTime lastSpeechTime;
    
    // æ€§èƒ½ä¼˜åŒ–å‚æ•° - æ–°å¢
    private const int MAX_HISTORY_LENGTH = 50; // ä¸¥æ ¼é™åˆ¶å†å²é•¿åº¦
    private const int MAX_SPEECH_EVENTS = 20; // é™åˆ¶è¯­éŸ³äº‹ä»¶å†å²
    private float lastCleanupTime = 0f;
    private const float CLEANUP_INTERVAL = 30f; // 30ç§’æ¸…ç†ä¸€æ¬¡

    void Awake()
    {
        InitializeComponents();
        InitializeAudioBuffer();
        currentVoiceEmotion = new EmotionDetectionSystem.EmotionData();
        lastAnalysisTime = DateTime.Now;
        
        // åˆå§‹åŒ–å™ªå£°åŸºçº¿
        StartCoroutine(CalibrateNoiseFloor());
    }

    void Start()
    {
        SetupVoiceProcessorEvents();
        
        if (showAnalysisLogs)
            Debug.Log("ğŸ¤ Voice Emotion Analyzer initialized with enhanced VAD");
    }

    void Update()
    {
        if (!enableVoiceAnalysis) return;
        
        // å®šæœŸæ¸…ç†å†å²æ•°æ®
        if (Time.time - lastCleanupTime >= CLEANUP_INTERVAL)
        {
            CleanupHistoryData();
            lastCleanupTime = Time.time;
        }
        
        // å®šæœŸåˆ†æéŸ³é¢‘æ•°æ®
        AnalyzeAudioFeatures();
        
        // æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
        UpdateDataValidity();
        
        // å¿«æ·é”®å¼ºåˆ¶å¯ç”¨UI
        if (Input.GetKeyDown(KeyCode.F2))
        {
            ForceEnableUI();
        }
    }
    
    /// <summary>
    /// å™ªå£°åŸºçº¿æ ¡å‡†åç¨‹
    /// </summary>
    private System.Collections.IEnumerator CalibrateNoiseFloor()
    {
        yield return new WaitForSeconds(2f); // ç­‰å¾…2ç§’è¿›è¡Œæ ¡å‡†
        
        if (volumeHistory.Count > 10)
        {
            // ä½¿ç”¨æœ€ä½çš„10%éŸ³é‡ä½œä¸ºå™ªå£°åŸºçº¿
            var sortedVolumes = new List<float>(volumeHistory);
            sortedVolumes.Sort();
            int calibrationCount = Mathf.Max(1, sortedVolumes.Count / 10);
            noiseFloor = sortedVolumes.Take(calibrationCount).Average();
            
            if (showAnalysisLogs)
                Debug.Log($"ğŸ¤ Noise floor calibrated: {noiseFloor:F4}");
        }
    }
    
    /// <summary>
    /// æ¸…ç†å†å²æ•°æ®é˜²æ­¢å†…å­˜æ³„æ¼
    /// </summary>
    private void CleanupHistoryData()
    {
        // é™åˆ¶å„ç§å†å²æ•°æ®çš„é•¿åº¦
        TrimList(volumeHistory, MAX_HISTORY_LENGTH);
        TrimList(pitchHistory, MAX_HISTORY_LENGTH);
        TrimList(speechRateHistory, MAX_HISTORY_LENGTH);
        TrimList(voiceActivityHistory, MAX_HISTORY_LENGTH);
        
        // æ¸…ç†è¿‡æœŸçš„è¯­éŸ³äº‹ä»¶
        DateTime cutoffTime = DateTime.Now.AddSeconds(-dataValidityPeriod * 2);
        speechEventTimes.RemoveAll(time => time < cutoffTime);
        
        // é™åˆ¶è¯­éŸ³äº‹ä»¶æ€»æ•°
        if (speechEventTimes.Count > MAX_SPEECH_EVENTS)
        {
            speechEventTimes.RemoveRange(0, speechEventTimes.Count - MAX_SPEECH_EVENTS);
        }
        
        if (showAnalysisLogs)
        {
            Debug.Log($"ğŸ¤ Cleaned up history data. Counts: Vol={volumeHistory.Count}, " +
                     $"Pitch={pitchHistory.Count}, Events={speechEventTimes.Count}");
        }
    }
    
    /// <summary>
    /// é™åˆ¶åˆ—è¡¨é•¿åº¦çš„è¾…åŠ©æ–¹æ³•
    /// </summary>
    private void TrimList<T>(List<T> list, int maxLength)
    {
        while (list.Count > maxLength)
        {
            list.RemoveAt(0);
        }
    }
    
    /// <summary>
    /// åˆå§‹åŒ–ç»„ä»¶
    /// </summary>
    private void InitializeComponents()
    {
        if (voiceProcessor == null)
            voiceProcessor = FindObjectOfType<VoiceProcessor>();
            
        if (voskSpeech == null)
            voskSpeech = FindObjectOfType<VoskSpeechToText>();
    }
    
    /// <summary>
    /// åˆå§‹åŒ–éŸ³é¢‘ç¼“å†²åŒº
    /// </summary>
    private void InitializeAudioBuffer()
    {
        audioBuffer = new float[bufferSize];
    }
    
    /// <summary>
    /// è®¾ç½®è¯­éŸ³å¤„ç†å™¨äº‹ä»¶
    /// </summary>
    private void SetupVoiceProcessorEvents()
    {
        if (voiceProcessor != null)
        {
            voiceProcessor.OnFrameCaptured += OnAudioFrameCaptured;
        }
        
        if (voskSpeech != null)
        {
            voskSpeech.OnTranscriptionResult += OnSpeechRecognized;
        }
    }
    
    /// <summary>
    /// éŸ³é¢‘å¸§æ•è·å›è°ƒ
    /// </summary>
    private void OnAudioFrameCaptured(short[] audioData)
    {
        if (!enableVoiceAnalysis || audioData == null || audioData.Length == 0)
            return;
        
        // è½¬æ¢ä¸ºfloatæ•°ç»„å¹¶å­˜å‚¨
        int copyLength = Mathf.Min(audioData.Length, audioBuffer.Length);
        for (int i = 0; i < copyLength; i++)
        {
            audioBuffer[i] = audioData[i] / (float)short.MaxValue;
        }
        
        // ç«‹å³åˆ†æå½“å‰å¸§
        AnalyzeCurrentFrame();
        hasRecentData = true;
        lastAnalysisTime = DateTime.Now;
    }
    
    /// <summary>
    /// è¯­éŸ³è¯†åˆ«ç»“æœå›è°ƒ
    /// </summary>
    private void OnSpeechRecognized(string recognizedText)
    {
        if (!enableVoiceAnalysis) return;
        
        lastRecognizedText = recognizedText;
        lastSpeechTime = DateTime.Now;
        speechEventTimes.Add(DateTime.Now);
        
        // è®¡ç®—è¯­é€Ÿ
        CalculateSpeechRate();
        
        // åˆ†æè¯­éŸ³å†…å®¹æƒ…æ„Ÿï¼ˆåŸºäºå…³é”®è¯ï¼‰
        AnalyzeSpeechContent(recognizedText);
        
        if (showAnalysisLogs)
        {
            Debug.Log($"ğŸ¤ Speech recognized: {recognizedText}, Current speech rate: {currentSpeechRate:F2}");
        }
    }
    
    /// <summary>
    /// åˆ†æå½“å‰éŸ³é¢‘å¸§ - ä¼˜åŒ–ç‰ˆæœ¬ï¼ŒåŠ å…¥è¯­éŸ³æ´»åŠ¨æ£€æµ‹
    /// </summary>
    private void AnalyzeCurrentFrame()
    {
        // è®¡ç®—éŸ³é‡
        currentVolume = CalculateVolume();
        volumeHistory.Add(currentVolume);
        
        // è¯­éŸ³æ´»åŠ¨æ£€æµ‹ - æ›´æ™ºèƒ½çš„æ£€æµ‹
        bool isVoiceActive = DetectVoiceActivity(currentVolume);
        voiceActivityHistory.Add(isVoiceActive);
        
        // åªæœ‰åœ¨æ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³æ—¶æ‰è¿›è¡Œè¯¦ç»†åˆ†æ
        if (isVoiceActive && consecutiveVoiceFrames >= minSpeechFrames)
        {
            // è®¡ç®—åŸºé¢‘ï¼ˆéŸ³è°ƒä¼°è®¡ï¼‰
            currentPitch = EstimatePitch();
            pitchHistory.Add(currentPitch);
            
            // é¢‘è°±åˆ†æ
            PerformSpectrumAnalysis();
            
            // è®¡ç®—ç»Ÿè®¡ç‰¹å¾
            CalculateAudioStatistics();
        }
        else
        {
            // é™éŸ³æ—¶æ·»åŠ é»˜è®¤å€¼
            pitchHistory.Add(0f);
        }
        
        // ä¿æŒå†å²æ•°æ®æ•°é‡é™åˆ¶ï¼ˆåœ¨è¿™é‡Œåªæ˜¯ä¿é™©ï¼Œä¸»è¦æ¸…ç†åœ¨CleanupHistoryDataä¸­ï¼‰
        if (volumeHistory.Count > MAX_HISTORY_LENGTH * 2)
        {
            TrimList(volumeHistory, MAX_HISTORY_LENGTH);
            TrimList(pitchHistory, MAX_HISTORY_LENGTH);
            TrimList(voiceActivityHistory, MAX_HISTORY_LENGTH);
        }
    }
    
    /// <summary>
    /// è¯­éŸ³æ´»åŠ¨æ£€æµ‹ - æ–°å¢æ–¹æ³•
    /// </summary>
    private bool DetectVoiceActivity(float volume)
    {
        // åŠ¨æ€é˜ˆå€¼ï¼šå™ªå£°åŸºçº¿ + å›ºå®šé˜ˆå€¼
        float dynamicThreshold = noiseFloor + volumeThreshold;
        bool isAboveThreshold = volume > dynamicThreshold;
        
        if (isAboveThreshold)
        {
            consecutiveVoiceFrames++;
            consecutiveSilenceFrames = 0;
        }
        else
        {
            consecutiveSilenceFrames++;
            consecutiveVoiceFrames = 0;
        }
        
        // éœ€è¦è¿ç»­å¤šå¸§è¶…è¿‡é˜ˆå€¼æ‰è®¤ä¸ºæ˜¯è¯­éŸ³
        bool isValidVoice = consecutiveVoiceFrames >= minSpeechFrames;
        
        if (showAnalysisLogs && isValidVoice && consecutiveVoiceFrames == minSpeechFrames)
        {
            Debug.Log($"ğŸ¤ Voice activity detected! Volume: {volume:F4}, Threshold: {dynamicThreshold:F4}");
        }
        
        return isValidVoice;
    }
    
    /// <summary>
    /// è®¡ç®—éŸ³é‡ï¼ˆRMSï¼‰
    /// </summary>
    private float CalculateVolume()
    {
        float sum = 0f;
        for (int i = 0; i < audioBuffer.Length; i++)
        {
            sum += audioBuffer[i] * audioBuffer[i];
        }
        return Mathf.Sqrt(sum / audioBuffer.Length);
    }
    
    /// <summary>
    /// ä¼°è®¡åŸºé¢‘ï¼ˆç®€å•çš„è‡ªç›¸å…³æ–¹æ³•ï¼‰
    /// </summary>
    private float EstimatePitch()
    {
        // ç®€åŒ–çš„åŸºé¢‘ä¼°è®¡ - åœ¨å®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„ç®—æ³•
        const int minPeriod = 20;  // æœ€å°å‘¨æœŸ (å¯¹åº”é«˜é¢‘)
        const int maxPeriod = 200; // æœ€å¤§å‘¨æœŸ (å¯¹åº”ä½é¢‘)
        
        float maxCorrelation = 0f;
        int bestPeriod = minPeriod;
        
        for (int period = minPeriod; period < maxPeriod && period < audioBuffer.Length / 2; period++)
        {
            float correlation = 0f;
            for (int i = 0; i < audioBuffer.Length - period; i++)
            {
                correlation += audioBuffer[i] * audioBuffer[i + period];
            }
            
            correlation /= (audioBuffer.Length - period);
            
            if (correlation > maxCorrelation)
            {
                maxCorrelation = correlation;
                bestPeriod = period;
            }
        }
        
        // è½¬æ¢ä¸ºé¢‘ç‡ (Hz) - å‡è®¾é‡‡æ ·ç‡ä¸º16kHz
        float sampleRate = 16000f;
        return sampleRate / bestPeriod;
    }
    
    /// <summary>
    /// æ‰§è¡Œé¢‘è°±åˆ†æ
    /// </summary>
    private void PerformSpectrumAnalysis()
    {
        // ç®€åŒ–çš„é¢‘è°±åˆ†æ - è®¡ç®—ä¸åŒé¢‘æ®µçš„èƒ½é‡
        int spectrumSize = Mathf.Min(spectrum.Length, audioBuffer.Length / 2);
        
        // è®¡ç®—åŠŸç‡è°±ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        for (int i = 0; i < spectrumSize; i++)
        {
            if (i * 2 + 1 < audioBuffer.Length)
            {
                float real = audioBuffer[i * 2];
                float imag = i * 2 + 1 < audioBuffer.Length ? audioBuffer[i * 2 + 1] : 0;
                spectrum[i] = real * real + imag * imag;
            }
        }
        
        // è®¡ç®—ä¸åŒé¢‘æ®µçš„èƒ½é‡
        int lowEnd = spectrumSize / 8;     // ä½é¢‘æ®µ
        int midEnd = spectrumSize / 2;     // ä¸­é¢‘æ®µ
        
        lowFreqEnergy = 0f;
        midFreqEnergy = 0f;
        highFreqEnergy = 0f;
        
        for (int i = 0; i < lowEnd; i++)
            lowFreqEnergy += spectrum[i];
        for (int i = lowEnd; i < midEnd; i++)
            midFreqEnergy += spectrum[i];
        for (int i = midEnd; i < spectrumSize; i++)
            highFreqEnergy += spectrum[i];
        
        // å½’ä¸€åŒ–
        float totalEnergy = lowFreqEnergy + midFreqEnergy + highFreqEnergy;
        if (totalEnergy > 0)
        {
            lowFreqEnergy /= totalEnergy;
            midFreqEnergy /= totalEnergy;
            highFreqEnergy /= totalEnergy;
        }
    }
    
    /// <summary>
    /// è®¡ç®—éŸ³é¢‘ç»Ÿè®¡ç‰¹å¾
    /// </summary>
    private void CalculateAudioStatistics()
    {
        if (volumeHistory.Count > 0)
        {
            averageVolume = volumeHistory.Average();
            
            // è®¡ç®—éŸ³é‡æ–¹å·®ï¼ˆæƒ…æ„Ÿå¼ºåº¦æŒ‡æ ‡ï¼‰
            float sumSquaredDiff = 0f;
            foreach (float vol in volumeHistory)
            {
                float diff = vol - averageVolume;
                sumSquaredDiff += diff * diff;
            }
            volumeVariance = sumSquaredDiff / volumeHistory.Count;
        }
    }
    
    /// <summary>
    /// è®¡ç®—è¯­é€Ÿ
    /// </summary>
    private void CalculateSpeechRate()
    {
        // æ¸…ç†è¿‡æœŸçš„è¯­éŸ³äº‹ä»¶
        DateTime cutoffTime = DateTime.Now.AddSeconds(-10); // 10ç§’çª—å£
        speechEventTimes.RemoveAll(time => time < cutoffTime);
        
        // è®¡ç®—è¯­é€Ÿï¼ˆæ¯åˆ†é’Ÿå•è¯æ•°çš„ç®€åŒ–ç‰ˆæœ¬ï¼‰
        if (speechEventTimes.Count >= 2)
        {
            var recentEvents = speechEventTimes.TakeLast(speechRateWindow).ToList();
            if (recentEvents.Count >= 2)
            {
                TimeSpan timeSpan = recentEvents.Last() - recentEvents.First();
                if (timeSpan.TotalSeconds > 0)
                {
                    currentSpeechRate = (recentEvents.Count - 1) / (float)timeSpan.TotalMinutes;
                }
            }
        }
        
        speechRateHistory.Add(currentSpeechRate);
        if (speechRateHistory.Count > 20)
            speechRateHistory.RemoveAt(0);
    }
    
    /// <summary>
    /// åˆ†æè¯­éŸ³å†…å®¹çš„æƒ…æ„Ÿå€¾å‘
    /// </summary>
    private void AnalyzeSpeechContent(string text)
    {
        if (string.IsNullOrEmpty(text)) return;
        
        text = text.ToLower();
        
        // ç®€å•çš„å…³é”®è¯æƒ…æ„Ÿåˆ†æ
        float contentValence = 0f;
        float contentArousal = 0f;
        
        // æ­£é¢è¯æ±‡
        string[] positiveWords = { "good", "great", "excellent", "amazing", "wonderful", "happy", "love", "like", "yes", "å¥½", "æ£’", "å¾ˆå¥½", "å–œæ¬¢" };
        // è´Ÿé¢è¯æ±‡
        string[] negativeWords = { "bad", "terrible", "awful", "hate", "no", "stop", "wrong", "error", "å", "ç³Ÿç³•", "ä¸å¥½", "è®¨åŒ", "åœæ­¢" };
        // é«˜å…´å¥‹åº¦è¯æ±‡
        string[] excitedWords = { "wow", "amazing", "incredible", "fantastic", "excited", "å“‡", "å¤ªæ£’äº†", "å…´å¥‹", "æ¿€åŠ¨" };
        
        foreach (string word in positiveWords)
        {
            if (text.Contains(word))
                contentValence += 0.2f;
        }
        
        foreach (string word in negativeWords)
        {
            if (text.Contains(word))
                contentValence -= 0.2f;
        }
        
        foreach (string word in excitedWords)
        {
            if (text.Contains(word))
                contentArousal += 0.3f;
        }
        
        // æ›´æ–°å½“å‰æƒ…æ„Ÿæ•°æ®çš„å†…å®¹åˆ†æéƒ¨åˆ†
        currentVoiceEmotion.valence = Mathf.Lerp(currentVoiceEmotion.valence, contentValence, 0.3f);
        currentVoiceEmotion.arousal = Mathf.Lerp(currentVoiceEmotion.arousal, contentArousal, 0.3f);
    }
    
    /// <summary>
    /// åˆ†æéŸ³é¢‘ç‰¹å¾
    /// </summary>
    private void AnalyzeAudioFeatures()
    {
        if (!hasRecentData || volumeHistory.Count < 10) return;
        
        var emotion = new EmotionDetectionSystem.EmotionData();
        
        // åŸºäºéŸ³é‡åˆ†æå…´å¥‹åº¦ (arousal)
        float volumeArousal = 0f;
        if (averageVolume > volumeThreshold)
        {
            volumeArousal = Mathf.Clamp01((averageVolume - volumeThreshold) * 2f);
            // éŸ³é‡å˜åŒ–å¤§è¡¨ç¤ºæƒ…æ„Ÿå¼ºçƒˆ
            volumeArousal += Mathf.Clamp01(volumeVariance * 10f);
        }
        
        // åŸºäºé«˜é¢‘èƒ½é‡åˆ†æå…´å¥‹åº¦
        float freqArousal = Mathf.Clamp01(highFreqEnergy * 2f);
        
        // åŸºäºè¯­é€Ÿåˆ†æå…´å¥‹åº¦
        float speechArousal = 0f;
        if (speechRateHistory.Count > 0)
        {
            float avgSpeechRate = speechRateHistory.Average();
            speechArousal = Mathf.Clamp01((avgSpeechRate - 60f) / 120f); // æ­£å¸¸è¯­é€Ÿ60è¯/åˆ†é’Ÿ
        }
        
        // ç»¼åˆå…´å¥‹åº¦
        emotion.arousal = (volumeArousal * 0.4f + freqArousal * 0.3f + speechArousal * 0.3f);
        
        // åŸºäºéŸ³è°ƒåˆ†ææƒ…æ„Ÿä»·å€¼ (valence)
        float pitchValence = 0f;
        if (pitchHistory.Count > 5)
        {
            float avgPitch = pitchHistory.TakeLast(10).Average();
            float pitchVariance = 0f;
            var recentPitches = pitchHistory.TakeLast(10);
            foreach (float pitch in recentPitches)
            {
                float diff = pitch - avgPitch;
                pitchVariance += diff * diff;
            }
            pitchVariance /= recentPitches.Count();
            
            // è¾ƒé«˜çš„éŸ³è°ƒé€šå¸¸è¡¨ç¤ºç§¯ææƒ…æ„Ÿ
            if (avgPitch > 150f) // åŸºçº¿é¢‘ç‡
            {
                pitchValence = Mathf.Clamp01((avgPitch - 150f) / 200f);
            }
            else
            {
                pitchValence = -Mathf.Clamp01((150f - avgPitch) / 100f);
            }
            
            // éŸ³è°ƒå˜åŒ–å¤§å¯èƒ½è¡¨ç¤ºæƒ…æ„Ÿä¸°å¯Œ
            emotion.intensity = Mathf.Clamp01(pitchVariance / 1000f);
        }
        
        emotion.valence = pitchValence;
        
        // è®¡ç®—ç½®ä¿¡åº¦
        float dataQuality = Mathf.Clamp01(averageVolume / 0.5f);
        float historyQuality = Mathf.Clamp01(volumeHistory.Count / 50f);
        emotion.confidence = (dataQuality + historyQuality) * 0.5f;
        
        // æƒ…æ„Ÿå¼ºåº¦åŸºäºå¤šä¸ªå› ç´ 
        emotion.intensity = Mathf.Clamp01((volumeVariance * 10f + emotion.arousal + Mathf.Abs(emotion.valence)) / 3f);
        
        // æ›´æ–°å½“å‰æƒ…æ„ŸçŠ¶æ€
        currentVoiceEmotion = emotion;
        
        // è§¦å‘æƒ…æ„Ÿæ£€æµ‹äº‹ä»¶
        OnVoiceEmotionDetected?.Invoke(emotion);
        
        if (showAnalysisLogs && emotion.confidence > 0.3f)
        {
            Debug.Log($"ğŸ¤ Voice emotion analysis: Arousal={emotion.arousal:F2}, Valence={emotion.valence:F2}, Intensity={emotion.intensity:F2}, Confidence={emotion.confidence:F2}");
        }
    }
    
    /// <summary>
    /// æ›´æ–°æ•°æ®æœ‰æ•ˆæ€§
    /// </summary>
    private void UpdateDataValidity()
    {
        TimeSpan timeSinceLastData = DateTime.Now - lastAnalysisTime;
        hasRecentData = timeSinceLastData.TotalSeconds < dataValidityPeriod;
        
        if (!hasRecentData && showAnalysisLogs)
        {
            Debug.Log("ğŸ¤ Voice data expired, waiting for new audio input...");
        }
    }
    
    // === Public Interface ===
    
    /// <summary>
    /// æ˜¯å¦æœ‰æœ€è¿‘çš„æœ‰æ•ˆæ•°æ®
    /// </summary>
    public bool HasRecentData()
    {
        return hasRecentData && enableVoiceAnalysis;
    }
    
    /// <summary>
    /// è·å–å½“å‰è¯­éŸ³æƒ…æ„Ÿæ•°æ®
    /// </summary>
    public EmotionDetectionSystem.EmotionData GetCurrentEmotion()
    {
        return currentVoiceEmotion;
    }
    
    /// <summary>
    /// è·å–å½“å‰éŸ³é¢‘ç‰¹å¾
    /// </summary>
    public (float volume, float pitch, float speechRate) GetCurrentAudioFeatures()
    {
        return (averageVolume, currentPitch, currentSpeechRate);
    }
    
    /// <summary>
    /// é‡ç½®åˆ†ææ•°æ®
    /// </summary>
    public void ResetAnalysisData()
    {
        volumeHistory.Clear();
        pitchHistory.Clear();
        speechRateHistory.Clear();
        speechEventTimes.Clear();
        voiceActivityHistory.Clear(); // æ–°å¢
        
        currentVolume = 0f;
        currentPitch = 0f;
        currentSpeechRate = 0f;
        averageVolume = 0f;
        volumeVariance = 0f;
        
        // é‡ç½®è¯­éŸ³æ´»åŠ¨æ£€æµ‹çŠ¶æ€
        consecutiveSilenceFrames = 0;
        consecutiveVoiceFrames = 0;
        
        currentVoiceEmotion = new EmotionDetectionSystem.EmotionData();
        hasRecentData = false;
        
        if (showAnalysisLogs)
            Debug.Log("ğŸ¤ Voice analysis data reset with VAD state");
    }
    
    /// <summary>
    /// å¼ºåˆ¶å¯ç”¨UIæ˜¾ç¤º - ç”¨äºè°ƒè¯•
    /// </summary>
    public void ForceEnableUI()
    {
        showVoiceUI = true;
        showAnalysisLogs = true;
        
        Debug.Log("ğŸ¤ Voice UI forcefully enabled for debugging");
    }
    
    /// <summary>
    /// ç®€å•çš„UIæ˜¾ç¤º
    /// </summary>
    void OnGUI()
    {
        if (!showVoiceUI) return;
        
        GUI.Box(new Rect(Screen.width - 320, 10, 300, 160), "Voice Analysis Status");
        
        // æ˜¾ç¤ºåŸºæœ¬çŠ¶æ€
        GUI.Label(new Rect(Screen.width - 310, 35, 280, 20), $"Analysis Enabled: {enableVoiceAnalysis}");
        GUI.Label(new Rect(Screen.width - 310, 55, 280, 20), $"Has Recent Data: {hasRecentData}");
        
        if (hasRecentData)
        {
            // æ˜¾ç¤ºè¯¦ç»†æ•°æ®
            GUI.Label(new Rect(Screen.width - 310, 75, 280, 20), $"Volume: {averageVolume:F3}");
            GUI.Label(new Rect(Screen.width - 310, 95, 280, 20), $"Pitch: {currentPitch:F1} Hz");
            GUI.Label(new Rect(Screen.width - 310, 115, 280, 20), $"Speech Rate: {currentSpeechRate:F1} words/minute");
            GUI.Label(new Rect(Screen.width - 310, 135, 280, 20), $"Emotion Intensity: {currentVoiceEmotion.intensity:F2}");
            GUI.Label(new Rect(Screen.width - 310, 155, 280, 20), $"Confidence: {currentVoiceEmotion.confidence:F2}");
        }
        else
        {
            // æ˜¾ç¤ºç­‰å¾…çŠ¶æ€
            GUI.Label(new Rect(Screen.width - 310, 75, 280, 20), "Waiting for voice input...");
            GUI.Label(new Rect(Screen.width - 310, 95, 280, 20), $"Volume Threshold: {volumeThreshold:F3}");
            GUI.Label(new Rect(Screen.width - 310, 115, 280, 20), $"Noise Floor: {noiseFloor:F4}");
            GUI.Label(new Rect(Screen.width - 310, 135, 280, 20), $"Voice Frames Needed: {minSpeechFrames}");
            
            // æ˜¾ç¤ºå½“å‰éŸ³é‡ï¼ˆå³ä½¿æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼‰
            if (volumeHistory.Count > 0)
            {
                float currentVol = volumeHistory.LastOrDefault();
                GUI.Label(new Rect(Screen.width - 310, 155, 280, 20), $"Current Volume: {currentVol:F4}");
            }
        }
    }
    
    void OnDestroy()
    {
        // å–æ¶ˆäº‹ä»¶è®¢é˜…
        if (voiceProcessor != null)
        {
            voiceProcessor.OnFrameCaptured -= OnAudioFrameCaptured;
        }
        
        if (voskSpeech != null)
        {
            voskSpeech.OnTranscriptionResult -= OnSpeechRecognized;
        }
    }
} 